{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAcxnaBNbUjSE/c2NH/rrp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seungmindavid/Machine-Learning-2023/blob/main/Pillars_of_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pillars of Machine Learning\n",
        "\n",
        "1.   Linear Algebra\n",
        "2.   Calculus\n",
        "3.   Probability Theory\n",
        "\n"
      ],
      "metadata": {
        "id": "D-33rCIXGXWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Algebra\n",
        "\n",
        "About Norms\n",
        "* Notation for norm of vector x: ||x||\n",
        "\n",
        "https://builtin.com/data-science/vector-norms\n",
        "\n",
        "Many applications like information retrieval, personalization, document categorization, image processing, and so on, rely on the computation of similarity or dissimilarity between items. Two items are considered similar if the distance between them is small, and vice versa.\n",
        "\n",
        "So, how de we calculate this distance?  -> Norm\n",
        "A Norm is a way to measure the size of a vector, a matrix, or a tensor. In other words, norms are a class of functions that enable us quantify the magnitude of a vector.\n",
        "\n",
        "\n",
        "\n",
        "The subject of norms comes up on many occasions in the context of machine learning:\n",
        "1. When defining loss functions, i.e., the distance between the actual and predicted values\n",
        "2. As a regularization method in machine learning, e.g., ridge and lasso regularization methods\n",
        "3. Even algorithms like SVM (Support Vector Machine) use the concept of the norm to calculate the distance between the discriminant and each support vector.\n",
        "\n",
        "\n",
        "What are the properties of Norm?\n",
        "- Consider two vectorx X and Y, having the same size and a scalar. A function is considered a norm iff it satisfies the following properties.\n",
        "1. Non-negativity: It should always be non-negative: $||x|| \\geq 0$\n",
        "2. Definiteness: It is zero iff the vector is zero, i.e., zero vector: $||x|| = 0 \\iff x = 0$\n",
        "3. Triangle inequality: The norm of a sum of two vectors is no more than the sum of their norms : $||X+Y|| \\leq ||x|| + ||y||$\n",
        "4. Homogeneity: Multiplying a vector by a scalar multiplies the norm of the vector by the absolute value of the scalar.: $||λ ⋅ x|| = ||\\lambda|| \\cdot ||x||$, where $\\lambda \\in \\mathbb{R}$.\n",
        "\n",
        "L1 Norm: Manhattan Norm\n",
        "$||x||_1 = |x_1| + |x_2|$\n",
        "\n",
        "L2 Norm: Euclidean Norm\n",
        "The L2 Norm is the most commonly used one in machine learning. Since it entails squaring of each component of the vector, it is not robust to outliers.\n",
        "\n",
        "\n",
        "\n",
        "# Vector Decomposition rules\n",
        "\n",
        "1. Vector u is orthogonal to vector v (meaning u·v = 0)\n",
        "2. Vector v is aligned with b (they have a zero angle)\n",
        "\n",
        "Then, let's say $a, b ∈ R^d$ two d-dimensional vectors. Let's decompose a into u and v s.t,\n",
        "1) a = u + v\n",
        "2) u is orthogonal to v\n",
        "3) v is aligned with b\n",
        "\n",
        "Let's first get v. Since v is aligned with b, we need projection of a onto b.\n",
        "Projection of a onto b: $v = (\\frac{(a·b)}{||b||^2})·b$\n",
        "\n",
        "And u is orthogonal to v. Hence, $u = a-v$\n",
        "\n"
      ],
      "metadata": {
        "id": "tlrQgs9Y_Agr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QD39b3I7_S8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Vector Calculus\n",
        "\n",
        "1. If $f(x) = w^{T}x$ then $\\nabla_xf(x)=w$.\n",
        "\n",
        "$f(x) = w^{T}x = w_1x_1+w_2x_2+ ... +w_dx_d$. Therefore, $\\nabla_xf(x) = w$\n",
        "\n",
        "\n",
        "2. If $f(x) = x^{T}Ax$ then $\\nabla_xf(x)=2Ax$\n",
        "\n",
        "If $f(x) = x^Tx$, then $\\nabla_xf(x) = 2x$, since $x^Tx = ∑^N_{i=1}x_i^2$. Then, when $\\nabla_x(x^TAx) = 2Ax $\n",
        "\n"
      ],
      "metadata": {
        "id": "5wVxzQAiG8LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RW7ZbG7GG8y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability Densities\n",
        "\n",
        "\n",
        "### Gaussian Distribution with Uni-variate & Multi-variate\n",
        "\n",
        "\n",
        "*   $x \\in \\mathbb{R}$: Univariate\n",
        "\n",
        "\n",
        "Gaussian distribution: $p(x)$, with mean $\\mu \\in \\mathbb{R}$ and variance $\\sigma \\in \\mathbb{R}$\n",
        "\n",
        "$p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{x-\\mu}{2\\sigma^2}}$\n",
        "\n",
        "*   $x \\in \\mathbb{R}^{d}$: Multivariate\n",
        "\n",
        "Multivariate Gaussian distribution: $p(x)$, with mean $\\mu \\in \\mathbb{R}^d$ and covariance $\\Sigma \\in \\mathbb{R}^{d\\times d}$.\n",
        "\n",
        "$p(x) = \\frac{1}{(2\\pi)^{\\frac{d}{2}} |\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}$\n",
        "\n",
        "\n",
        "\n",
        "### Gradient and Hessian\n",
        "When $x \\in \\mathbb{R}^{d}$, $p(x)$ be the multivariate Gaussian, and $f(x) = -\\log(p(x))$.\n",
        "\n",
        "$\\nabla_xf(x) = \\nabla(-\\log(\\frac{1}{(2\\pi)^{\\frac{d}{2}} |\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}))$\n",
        "\n",
        "$= \\nabla_xf(x) = \\nabla(-log(\\frac{1}{(2\\pi)^{\\frac{d}{2}}|\\Sigma|^{\\frac{1}{2}}})-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu))$\n",
        "\n",
        "= $\\nabla_x(-((\\log(1)-\\log((2\\pi)^{\\frac{d}{2}}(|\\Sigma|^{\\frac{1}{2}}))-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1}(x-\\mu))$\n",
        "\n",
        "= $\\nabla_x(\\frac{d}{2}log(2\\pi)+\\frac{1}{2}log(|\\Sigma|)+\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu))$\n",
        "\n",
        "= $\\frac{1}{2} \\cdot 2|\\Sigma|^{-1}(x-\\mu) = |\\Sigma|^{-1} (x-\\mu)$, by using the rule $\\nabla_x(x^{T}Ax) = 2Ax$.\n",
        "\n",
        "Then, our Hessian is: $|\\Sigma|^{-1}$.\n",
        "\n"
      ],
      "metadata": {
        "id": "9viPhwwvG-6a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vo9UD2H_HFbk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}